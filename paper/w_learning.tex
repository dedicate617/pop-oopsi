%In order to perform maximization step of EM, the expectation of the log-likehood  Eq. \eqref{eqn:loglik:definition-expl} needs to be maximized with respect to $\bth^F=\{\bth^F_i\}$ and $\bth^n = \{\bth^n_i\} = (\bw_i, b_i)$. For $N$ neurons this is a very large optimization problem with $6N$ parameters $M$ and $m N^2 + N$ parameters $W$.  Fortunately, this optimization problem admits dramatic simplifications making it tractable for existing computers.  Specifically, estimation of parameters $M_i$ may be performed individually for each neuron since calcium dynamics of different neurons are independent from each other and, given $H$, also decoupled from GLM. Finding parameters $M_i$, thus, only involves solving of $N$ 6D-optimization subproblems (see \cite{Vogelstein2009} for details).

Computing the maximum likelihood estimates of the functional connectivity matrix, $\bw$, is an optimization problem with $N^2$ variables. By construction, however, log-likelihood is convex in $\bw$, and, log-likelihoods for different neurons, Eq. \eqref{eqn:likelihoodGLM-expl}, are independent and may be maximized separately. Estimating $\bw$ thus involves solving of $N$ separable $N$-dimensional convex optimization subproblems, which can be done efficiently using standard algorithms such as Newton-Rapson methods. Below we provide details for estimating $\bw$ given the two different strategies for obtaining the joint posteriors, as well as a couple extensions.

\subsubsection{Estimating the functional connectivity when using the independent approximation to estimate the joint posteriors}

As indicated by Eq. \eqref{eqn:}


E.g., we used standard Matlab's nonlinear optimization function \texttt{fmincon}, provided in optimization toolbox, to solve this problem for up to $N=500$ neurons.

Simple properties of the connectivity matrix, that may be known a priori, may be extremely helpful in obtaining accurate solutions for smaller datasets. In particular, enforcing sparseness for signal recovered with a series of linear measurements via $L1$-regularizer is known to dramatically reduce the amount of data necessary to accurately reconstruct the signal \cite{Candes2005, DE03, Mishchenko2009}. Although here the estimation problem is not linear, it is interesting what impact analogous prior might have on the reconstruction of the matrix of functional connection weights $W$. Enforcing sparseness may be done by introducing exponential prior to GLM \cite{Stevenson2009}, thus leading to the posterior probability for the spike train

\begin{equation}\label{eqn:likelihoodsparseGLM}
E[\ln P_{\bn}(n_i|{\bf n}_{\i}, W)P[W]]=\sum_t E\left( n_i(t) J_i(t) - (1-n_i(t)) \exp(J_i(t)) \Delta \right)-\lambda \sum_{t'ij}|\w_{ij}(t')|.
\end{equation}

Exponential prior parameter $\lambda\sim 1/\langle|\w_{ij}|\rangle$ may be set from a priori neuroanatomical or neurophysiological data. Sparse prior does not change the convexity of GLM log-likelihood and, so, such regularized problem may be solved efficiently using methods of convex optimization theory. E.g., by introducing slack variables $s_{ij}(t)>|\w_{ij}(t)|$, this problem may be converted to a nonlinear program that can be solved using interior point method

\begin{equation} \label{eqn:conconvexopt}
\begin{array}{l}
\min \left\{-\sum\limits_t E\left[ n_i(t) J_i(t) - (1-n_i(t)) \exp(J_i(t)) \Delta \right]+\lambda \sum\limits_{t'j}s_{ij}(t')\right\}, \text{ s.t. } 
\\ \w_{ij}(t')<s_{ij}(t'), \; -\w_{ij}(t')<s_{ij}(t') \; \forall j, t'.
\end{array} 
\end{equation}

We used Matlab's standard function \text{fconmin}, provided in optimization toolbox, to solve this constrained optimization problem for up to $N=500$ neurons. As we will see below, sparse prior dramatically decreases the amount of data necessary for accurate estimation of the connectivity matrix.

Another property of the connectivity matrix that may be useful is the so called Dale's law. Dale's law is the empirical observation that neurons always make synapses of one kind, i.e. either inhibitory or excitatory. In terms of the connectivity matrix, Dale's law translates into the condition of sign-constancy of the matrix columns. Dale's law is easy to enforce by constraining $\w_{ij}$ to be either positive or negative for given $j$.  Sign assignments may be chosen by inspecting unconstrained solution $W$ and choosing the row $\w_{i \cdot}$ to be excitatory if the sum-squares of the positive terms $\w_{ij}$ for given $j$ in the unconstrained solution is greater than that of the negative terms, and inhibitory otherwise.  After that, Dale's law may be enforced either independently or together with the sparse prior.  The nonlinear program in the latter case becomes

\begin{equation}
\begin{array}{l}
\min \left\{-\sum\limits_t E\left[ n_i(t) J_i(t) - (1-n_i(t) \exp(J_i(t)) \Delta \right]+\lambda \sum\limits_{t'j}s_{ij}(t')\right\}, \text{ s.t. }\\
\w_{ij}(t')<0, \; -\w_{ij}(t')<s_{ij}(t') \; \forall j, t'\text{ where }j\text{ is inhibitory neuron}, \\
\w_{ij}(t')<s_{ij}(t'), \; -\w_{ij}(t')<0 \; \forall j, t'\text{ where }j\text{ is excitatory neuron}. \\
\end{array}
\end{equation}

This optimization problem is essentially equivalent to the constrained optimization problem Eq. \eqref{eqn:conconvexopt}, and can be solved using the same methods. We used the same Matlab's function \texttt{fconmin} to solve this problem. Unlike sparse prior, Dale's prior did not lead to substantial improvement in the reconstructed connectivity matrix.
