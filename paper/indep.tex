As stated above, our goal here is to derive an algorithm to efficient estimate the marginal posterior over each neuron, $P[X(t), X(t-1) | F; \bth]$.  As this was discussed at length in \cite{Vogelstein2009}, here we only provide a brief overview.  The standard forward-backward equations provide these posteriors, assuming the below integrals can be evaluated:

\begin{align}
	P[X(t) | F(1), \ldots, F(t)] &= \frac{1}{Z} P[F(t)| X(t)] \int P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)] dX(t-1) \label{eqn:forward} \\
	P[X(t), X(t-1) | F] &= P[X(t) | F] \frac{P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)]}{\int P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)] dX(t-1)} \label{eqn:backward}
\end{align}

\noindent where we have dropped the subscript $i$ and the conditioning on the parameters for brevity sake.  Because the integral in Eq. \eqref{eqn:forward} cannot be analytically evaluated for our model, we approximate it with a sum using a sequential Monte Carlo (SMC) framework.  More specifically, we sample  from $x(t) \sim P[X(t) | X(t-1), F(1), \ldots, F(t)]$, and call each sample a ``particle''.  Given an entire particle swarm, we can compute the relative likelihood of each particle:

\begin{align}
	P[x(t) | F(1), \ldots, F(t)] = \frac{1}{Z}\frac{P[F(t) | x(t)] P[x(t) | x(t-1)] P[x(t-1) | F(1), \ldots, F(t-1)]} {P[x(t) | x(t-1), F(1), \ldots, F(t)]}
\end{align}

\noindent where $Z$ is a normalizing constant ensuring that $\sum P[x(t) | F(1), \ldots, F(t)]=1$.  Given these weights, we resample to reduce the variance of the particles.  Recursing these three steps (sample, compute weights, resample) for $t=1, \ldots, T$ completes the approximation to Eq. \eqref{eqn:forward}.  We can now plug these approximations into Eq. \eqref{eqn:backward}, to recursively obtain particle approximations to marginal posteriors over neurons, $P[X(t), X(t-1) | F]$.  Trivially, we can sum over $X(t)$ to get marginal posteriors of $X(t-1)$.

The sufficient statistics for estimating the parameters for each neuron, $\bth_i$, are these very marginal posteriors.  As shown in Eq. \eqref{eqn:loglik:definition-expl}, this maximization problem decouples into separate subproblems.  Specifically, the first term depends on only $\{\alpha_i, \beta_i, \gamma_i, \sigma_i\}$, which we can estimate by recursively solving a quadratic problem for $\{alpha_i, \beta_i\}$ while holding $\{\gamma_i, \sigma_i\}$ fixed, and then holding $\{alpha_i, \beta_i\}$ fixed, while estimating $\{gamma_i,\sigma_i\}$.  Considering only the second term, we can estimate $\{\tau_i^c, A_i, C_i^b\}$ again using a quadratic solver, and use the residuals to estimate $\sigma_i^c$.  Note that all the parameters mentioned so far are constrained to be non-negative, but may be solved very efficiently using Matlab's \texttt{quadprog}, providing the appropriate constraints, and the gradients and Hessians, if desirable.  Finally, the last term, assuming each neuron is independent, may be written:

\begin{multline}
	E [\ln P(n_i(t) | \bh_i(t))] = P[n_i(t) | F_i] \ln (1-\exp\{-\exp\{b_i + \w_{ii} h_i(t)\}\Delta\})
	\\ +  (1-P[n_i(t) | F_i]) (-\exp\{b_i + \w_{ii} h_i(t)\}\Delta)] 
\end{multline}

\noindent which is concave in $\{b_i, \w_{ii}\}$, and may therefore be solved efficiently using any gradient ascent solver. In practice, we've found that imposing constraints on $\{b_i, \w_{ii}\}$ improves both the robustness and efficiency of estimating these parameters.  Our rational is that the double exponential is such a strong nonlinearity, that as the absolute value of these parameters approaches $\approx 10$, the likelihood becomes extremely flat.  We therefore restrict these parameters to be within $[-5,15]$, using Matlab's \texttt{fmincon}.

Our procedure therefore is to initialize the parameters for each neuron using some default values that we've found to be effective in practice, and then recursively estimate the marginal posteriors (E step), and then maximize the parameters (M step), using the above described approach.  We iterate these two steps until the change in parameters does not exceed some minimum threshold, $\eta^n$.  We can then use the marginal posteriors from the last iteration to seed our Gibbs sampling procedure described below, to obtain an estimate of $P[\bX(t) | \bF]$.