To estimate $\ve{\omega}$, we adopt a state-space framework.  More specifically, let $y_t$ be the observations, and $x_t$ be the \emph{hidden states}.   We then have:

\begin{align}
\y_t &\sim \g (\y_t | \x_t) \label{eq:y_t} \\
\x_t &\sim \f (\x_t | \x_{t-1}) \label{eq:x_t}
\end{align}

\noindent where $\g(\cdot)$ and $\f(\cdot)$ indicate the \emph{observation} and \emph{transition} distributions, respectively (and we have adopted the shorthand notation $\f(\cdot)=f(\cdot | \thet)$.  Letting $\ve{X}_t$ = $\{X_{i,t}\}$ = $\{X_{1,t}, \ldots, X_{N,t}\}$, for the above model, we have $\y_t=\ve{F}_t$, $\x_t=\{\ve{n}_t, \ve{h}_t, [\textbf{Ca}^{2+}]_t\}$, and $\thet=\{\ve{\alpha}$, $\ve{\beta}$, $\ve{\tau_c}$, $\ve{\Ca_b}$, $\ve{\sigma_c}$, $\ve{b}$, $\{\ve{k}_i\}$, $\ve{\omega}$, $\ve{\tau_h}, \ve{\sigma}_h\}$.   Note that given the above model, $\y_t \in \Re_+^N$, i.e., each $y_t$ is a non-negative real number, and $\x_t \in \Re_+^{2N} \times \{0,1\}^N$, i.e., $\x_t$ lies in a $3N$-dimensional space, where $2N$ dimensions are non-negative real numbers ($\Ca_t$ and $h_t$ for each neuron), and the rest are binary ($n_t \in \{0,1\}$ for each neuron).  Thus, we have a ``hybrid'' system, in that some of our hidden states are continuous, and others are discrete.  We may now formally state our goal as:

\begin{align} \label{eq:max}
\widehat{\thet} = \argmax_{\thet} \p(\yT) = \argmax_{\thet} \int \p(\yT, \xT) d\xT 
\end{align}

\noindent where, we have introduced the notation, $\yT=\y_{0:T}$, and $\p(\yT, \xT)$ may be factored as: 

\begin{align}
\p(\yT, \xT) = \ppi (\x_0) \prod_{t=1}^T \g(\y_t | \x_t) \f(\x_t | \x_{t-1}) 
\end{align}

\noindent where $\ppi(\x_0)$ is the initial distribution of $\x$.  Sadly, for our model, integral in \eqref{eq:max} is computationally intractable. Therefore, we adopt an EM approach:

\begin{align}
\widehat{\thetn} &= \argmax_{\thetn} \Q\\
\Q &= E_{p_{\theto}(\xT | \yT)} \ln p_{\thetn} (\yT, \xT)% \nonumber \\ &
= \int p_{\theto} (\xT | \yT) \ln p_{\thetn} (\yT, \xT) d\xT 
\end{align}

Because we have a state-space model, the above integral may be simplified:

\begin{multline} \label{eq:Q2}
\Q = \int p_{\theto}(\x_0 | \yT)  \times \ln \pi_{\thetn}(\x_0) d\x_0 + \\
\sum_{t=1}^T \iint p_{\theto} (\x_t, \x_{t-1} | \yT) \times \ln f_{\thetn}(\x_t | \x_{t-1}) d\x_t d\x_{t-1} + \\
\sum_{t=0}^T \int p_{\theto} (\x_t | \yT) \times \ln g_{\thetn}(\y_t | \x_t) d\x_t
\end{multline}

As we have a nonlinear observation model, and nonlinear transition model, the integrals in \eqref{eq:Q2} are intractable as well, so they must be approximated. We adopt a monte carlo strategy, in which we approximate the above integrals with sums. This requires approximating the continuous valued hidden states with discrete valued hidden states. The key is to be able to estimate the pairwise joint conditionals:

\begin{align}
p_{\thet} (\x_t, \x_{t-1} | \yT) \approx \sum_{k,m=1}^k \widetilde{p}_{\thet} \big(\x_t^{(k)}, \x_{t-1}^{(m)} | \yT\big) \delta_{\x_t^{(k)}}(\x_t) \delta_{\x_{t-1}^{(m)}} (\x_{t-1})
\end{align}

\noindent where $\delta_{\x}(\cdot)$ denotes the Dirac mass at point $\x$, and $\widetilde{p}_{\thet}(\x_t^{(k)},  \x_{t-1}^{(m)} | \yT)$ is computed pointwise for each $\x_t^{(k)}$ and $\x_{t-1}^{(m)}$, and then normalized such that $\sum_{k,m=1}^k \widetilde{p}_{\thet}(\x_t^{(l)} | \x_{t-1}^{(m)})=1$. From this approximation, we can also estimate the marginal conditionals and initial distribution, by integrating out $\x_{t-1}$. The key is to efficiency discretize the space, so $L$ is small and the approximation is still good.  We adopt a Monte Carlo strategy, in which we represent the pairwise joint conditionals as a product of terms:

\begin{align}
 \phat \big(\x_t^{(l)}, \x_{t-1}^{(m)} | \yT\big) = \phat \big(\x_t^{(l)} | \yT\big) \frac{\fhat w_{t-1}^{(m)}} {\sum_{m=1}^L  \fhat w_{t-1}^{(m)}}
\end{align}

\noindent where $w_t^{(l)}$ is defined by

\begin{align}
\p(\x_{t} | \y_{1:t}) = \sum_{l=1}^L w_t^{(l)} \delta_{\x_{0:t}^{(l)}} (\x_{0:t})
\end{align}

The maximize efficiency, therefore, we aim to choose $\x_t^{(l)}$'s to make their associated weights, $w_t^{(l)}$'s approximately equal. In the next section, we describe a number of algorithms, with increasing complexity and (hopefully) utility. 

Thus, it should be clear from the above that ideally, one would ideally like to sample jointly from $p(\x_t | \x_{t-1}, \y_t)$.  As this is so high dimensional (more specifically, $3N$ dimensions), the importance sampling idea breaks down (as it is incapable of approximating a high dimensional space with a small number of particles).  Therefore, we develop a number of approaches to combat this curse of dimensionality.

%We utilize a Sequential Importance Resampling (SIR) strategy in which we choose as the importnace function the one-step-ahead sampler and perform stratified resampling as needed.  

%\subsection{single-neuron dynamics}
%
%Thus, we have a transition and observation distirbutions:
%
%\begin{align}
%p(\x_t | \x_{t-1}) &= p(\Ca_t | \Ca_{t-1}, n_t) p(n_t | h_t) p(h_t | h_{t-1}, n_{t-1})\\
%p(\y_t | \x_t) &= p(F_t | \Ca_t),
%\end{align}
%
%\noindent where the above distributions are defined as:
%
%\begin{align}
%p(\Ca_t | \Ca_{t-1}, n_t) &= \mathcal{N}(\Ca_t; a_c \Ca_{t-1} + b + A n_t; \sigma_c^2 \Delta) \\
%p(n_t | h_t) &= \mathcal{B}(n_t; \lambda_t) \\
%p(h_t | h_{t-1}, n_{t-1}) &= \mathcal{N}(h_t; a_h h_{t-1} + n_{t-1}, \sigma_h^2 \Delta)\\
%p(F_t | \Ca_t) &= \mathcal{N}(F_t; \alpha S(\Ca_t) + \beta, (S(\Ca_t) + \sigma_F)^2)
%\end{align}

\subsection{population dynamics}

plugging model described in section \ref{sec:popmod} into \eqref{eq:y_t} and \eqref{eq:x_t}, we have:

\begin{align}
p(\x_t | \x_{t-1}) &= \prod_{i=1}^N p(\Ca_{i,t} | \Ca_{i,t-1}, n_{i,t}) p(n_{i,t} | \h_t) p(h_{i,t} | h_{i,t-1}, n_{i,t-1})\\
p(\y_t | \x_t) &= \prod_{i=1}^N p(F_{i,t} | \Ca_{i,t})
\end{align}

\noindent where $\h_t=h_{1,t}, \ldots, h_{N,t}$ and the above distributions are defined by

\begin{align}
p(\Ca_{i,t} | \Ca_{i,t-1}, n_t) &= \mathcal{N}(\Ca_{i,t}; a_{c_i} \Ca_{i,t-1} + b_i + A_i n_{i,t}; \sigma_{c_i}^2 \Delta) \\
p(n_{i,t} | \h_{i,t}) &= \mathcal{B}(n_{i,t}; \lambda_{i,t}) \\
p(h_{i,t} | h_{i,t-1}, n_{i,t-1}) &= \mathcal{N}(h_{i,t}; a_{h_i} h_{i,t-1} + n_{i,t-1}, \sigma_{h_i}^2 \Delta)\\
p(F_{i,t} | \Ca_{i,t}) &= \mathcal{N}(F_{i,t}; \alpha_i S(\Ca_{i,t}) + \beta_i, (S(\Ca_{i,t}) + \sigma_{F_i})^2)
\end{align}

\noindent and we have introduced $a_{c_i}=1-\Delta/\tau_{c_i}$, $b_i=-\Delta \Ca_{b_i} / \tau_{c_i}$, and $a_{h_i}=1- \Delta / \tau_{h_i}$, for brevity.
