Our goal is to estimate the most likely connection matrix from a population of observable neurons, given only their calcium fluorescence observations.  We take a model based approach, meaning that we first describe a parametric generative model that completely characterizes the statistics of the data, and then we derive algorithms to learn the parameters, given the data.  

We use the following conventions throughout the paper, unless indicated otherwise.  Time is discrete, taking values $t=1,\ldots,T$.  We let $X_i(t)$ indicate the state of neuron $i$ at time $t$, $X_i=\{X_i(t), t=1,\ldots, T\}$, and ${\bf X}= \{X_1, \ldots, X_N\}$.  Conditional probability distributions will be written $P[\bf F | \bf X; \bth]$, where $\bf X$ indicates some random variables, $\bth$ indicates some parameters, and a semicolon separates the two. To indicate that a random variable, $X$, is independently and identically distributed according to some distribution $P$, we have $X \overset{iid}{\sim} P$.


\subsection{Model} 
\label{sec:methods:markov-setup}
We first describe the model that characterizes the statistics of the joint spike trains of all $N$ observable neurons.  Each neuron is modeled as a generalized linear model (GLM), which is known to capture well the statistical properties of the firing of individual neurons \cite{PILL07, PAN03d, Wu07, Rigat06, OKA05}.  More specifically, we say that at time $t$, the probability of neuron $i$ spiking is given by some nonlinear function, $f(\cdot)$, of the input to that neuron at that time, $J_i(t)$.  The input is composed of: (i) some baseline firing rate, $b_i$%, (ii) some external stimulus, $\bf S(t)$, that is linearly filtered by $\bf k_i$
, and (ii) spike history terms, $h_j(t)$, from each neuron $j$, weighted by $\w_{ij}$:

\begin{equation} \label{eqn:glm:definition}
\begin{array}{l}
n_i(t)\overset{iid}{\sim} \text{Bernoulli}\big(f\big(J_i(t)\big)\big), \qquad %\\
J_i(t)=b_i+\sum \limits_{j=1}^{N}  \w_{ij} h_{j}(t), %\\
% p_i(t)=f(J_i(t))=f(b_i+k_i\cdot S(t)+\sum_{j=1}^{N}  \w_{ij} h_{j}(t)), \\
%h_j(t) = (1- \Delta/\tau_h) h_j(t-1) +n_j(t) + \epsilon_{h_j}(t).
\end{array}
\end{equation}

\noindent To ensure mathematical tractability, we must impose some constraints on $f(\cdot)$ and the dynamics of $h_j(t)$.  More specifically, $f(\cdot)$ must be convex and log-concave, to ensure that the likelihood of the parameters of this model has a single maximum, facilitating efficient computations \cite{PAN03d}.  In all the below simulations, we let $f(J)=\text{e}^{\text{e}^{-J}\Delta}$, where the inclusion of $\Delta$, the time step size, ensures that the firing rate is independent of the particular time discretization of our model (see \cite{??} for a proof that this $f(\cdot)$ satisfies the above constraints).  Furthermore, as the algorithms we develop below assume Markovian dynamics, we model the spike history terms as:

\begin{equation} \label{eqn:h:definition}
h_j(t) = (1- \Delta/\tau_h) h_j(t-1) +n_j(t) + \sigma_h \sqrt{\Delta} \epsilon^h_j(t).
\end{equation}

\noindent where $\tau_h$ is the decay time constant for spike history terms, $\sigma_h$ is the standard deviation of noise, $\sqrt{\Delta}$ ensures that noise statistics are independent of the time discretization, and throughout this paper, $\epsilon_\cdot^\cdot$ is assumed to be an independent standard normal random variable, i.e., $\epsilon_{\cdot}^{\cdot}(t) \overset{iid}{\sim} \text{Normal}(0,1)$.  Note that this model generalizes straightforwardly to allow each neuron to have several spike history terms, each with a unique time constant, together imparting a wide variety of possible post-synaptic effects, including bursting, facilitating, and depressing synapses \cite{PAN03d}.  We assume that $\tau_h$ and $\sigma_h$ are known, and therefore our model spiking parameters $\bth^n=\{\bth^n_i\}_{i=1}^N$, where $\bth^n_i=\{\bw_i,b_i\}$, where $\bw_i=(\w_{i1},\ldots, \w_{iN})$.

The problem of estimating functional connectivity, given a model like the one above, when neural spikes $n_i(t)$ are assumed to be directly observed, has recently received much attention \cite{PILL07}. With calcium imaging, however, we do not directly observe spike trains. Instead, fluorescent signal from the calcium indicators conveys neural activity via hidden nonlinear calcium dynamics \cite{Vogelstein2009}: 

%\begin{equation}
\begin{align} \label{eqn:ca:definition} %{l}
C_i(t) &= C_i(t-1) + (C_i^b-C_i(t-1)) \Delta/\tau^c_i + A_i n_i(t)+\sigma^c_i \sqrt{\Delta} \epsilon^c_i(t), \\
F_i(t) &= \alpha_i S(C_i(t)) + \beta_i + \sqrt{\gamma_i S(C_i(t)) + \sigma^F_i} \epsilon^F_i(t). \label{eqn:F:definition}
\end{align}
% \end{equation}

Eq. \eqref{eqn:ca:definition} describes evolution of intracellular calcium concentration $C_i(t)$ in the neuron $i$ at time $t$. Under normal conditions, $C_i(t)$ fluctuates around the baseline level of $C_i^b$ with normally distributed noise $\epsilon^c_i(t)$ with standard deviation $\sigma^c_i \sqrt{\Delta}$.  Whenever the neuron fires a spike, $n_i(t)=1$, causing the calcium to jump by $A_i$, and subsequently decay with time constant $\tau^c_i$.  The fluorescence signal corresponding to neuron $i$ at time $t$, $F_i(t)$, corresponds to the count of photons collected at the detector per neuron per imaging frame. It is distributed according to normal statistics with the mean and variance given by generalized Hill functions, where $S(C)=C/(C+K_d)$ \cite{Yasuda2004}.  Because $K_d$ effectively scales the results, and is a property of the indicator, we assume throughout this work that it is known.  Therefore, each neuron has parameters $\bth_i=\{\w_{ii}, b_i, C^b_i, \tau^c_i, A_i, \sigma^c_i, \alpha_i, \beta_i, \gamma_i, \sigma^F_i\}$ independent of the other neurons.  The $\w_{ii}$'s make up the diagonal of the functional connection matrix, $\bw=\{\bw_i\}_{i=1}^N$, yielding a total of $|\bth|=(9+N)N$ parameters for our model.  Note that collectively Eqs. \eqref{eqn:glm:definition} -- \eqref{eqn:F:definition} defined a coupled hidden Markov model (HMM) \cite{ShumwayStoffer06}.


\subsection{Goal and general strategy}  \label{sec:methods:goal}

Given the above model, our goal is to estimate the functional connectivity matrix, $\bw$, given calcium imaging observations ${\bf F}$. A natural choice is find the \emph{maximum a posteriori} (MAP) estimate:

\begin{equation}\label{eqn:MAP}
\hbw=\argmax_{\bw} P[\bw| \bF] = \argmax_{\bw} \iint P[\bth| \bX, \bF) d\bX d(\bth \backslash \bw]
\end{equation}

\noindent where $\bth \backslash \bw$ is the set of parameters excluding the functional connectivity matrix.  Because directly solving Eq. \eqref{eqn:MAP} is intractable, we utilize the Expectation Maximization (EM) framework, in which one recursively updates the expected value of the joint distribution of $(\bX, \bF)$ (E step), and then maximizes all the parameters (M step):

\begin{align*}
\textbf{E step} &\text{: Evaluate } Q(\bth^{(l+1)},\bth^{(l)}) = E_{P[\bX | \bF; \bth^{(l+1)}]}[ \ln P[\bF, \bX | \bth^{(l)}]] = \int P[\bX | \bF; \bth^{(l+1)}] \ln P[\bF, \bX | \bth^{(l)}] d \bX  \\
\textbf{M step} &\text{: Solve } \bth^{(l+1)} = \argmax_{\bth} Q(\bth,\bth^{(l)})  
\end{align*}

Because our model is a coupled HMM, $Q$ simplifies:

\begin{multline}\label{eqn:loglik:definition-expl}
Q(\bth,\bth^{(l)}) = 
\sumit 
P[C_i(t) | F_i; \bth_i] \times \ln P[F_i(t)|C_i(t); \bth_i] 
\\ + P[C_i(t), C_i(t-1) | F_i; \bth_i] \times \ln P[C_i(t)|C_i(t-1), n_i(t); \bth_i] 
\\ + P[n_i(t), \bh(t) | \bF; \bth] \times \ln P[n_i(t)| \bh(t); \bth^n_i],
\end{multline}

\noindent where $\bh(t)=\{h_i(t)\}_{i=1}^N$.  Note that while the first two terms in Eq. \eqref{eqn:loglik:definition-expl} only require posteriors marginalized over each neuron, and all but nearest time bins, $P[X_i(t), X_i(t-1) | F_i; \bth_i]$, whereas the last term requires the joint posterior over all neurons (but marginalized over time), $P[\bX(t) | \bF; \bth^n]$.  

Unfortunately, analytic solutions for all these posteriors are intractable, so we are forced to use approximate methods to estimate them.  To estimate $P[X_i(t), X_i(t-1) | F_i; \bth_i]$ --- hereafter called ``marginal posteriors'' ---  we utilize a forward-backward procedure that discretize the integrals by sampling \cite{DFG01, MINKAPHD, Fearnhead2003, koyama08, Andrieu2007, NBR03}.  These sequential Monte Carlo (SMC) algorithms (or, ``particle filters'') operate very efficiently, scaling linearly with time \cite{RAB89}. However, as the dimensionality of the hidden state space increases, importance sampling becomes relatively inefficient.  For a population of about 50 neurons, the dimensionality of our model would be $3N=150$ --- too large for existing SMC methods \cite{??}.  

To solve this problem, we propose a hybrid Markov Chain Monte Carlo (MCMC) Gibbs sampling strategy taking advantage of the specific structure of the model Eqs. \eqref{eqn:glm:definition} and \eqref{eqn:h:definition}, namely, that it can be viewed as a set of $N$ coupled HMM models. Gibbs sampling is a procedure for obtaining samples from high-dimensional distributions $P[{\bf X}]$ by sampling from low-dimensional conditional distributions $P[X_{i}|{\bf X}_{\i}]$  \cite{Gelfand1990}.  This sampling procedure reduces intractable high-dimensional sampling problems to sequences of tractable low-dimensional subproblems.  Below, we propose to sample in blocks of one spike train at a time, using MCMC to sample entire spike trains.

The maximization step of EM requires maximizing the conditional expectation of $\ln P[{\bf X}, {\bf F} | \bth]$ given such samples. Although this is a maximization over $(9+N)N$ parameters, the special structure of Eq. \eqref{eqn:loglik:definition-expl} allows one to simplify this optimization problem dramatically by performing optimization with respect to each neuron independently.  

Our EM algorithm therefore requires solving three problems: (i) estimating marginal posteriors over neurons, $P[X_i(t), X_i(t-1) | F_i; \bth_i]$ using SMC, and using them to learn the most likely parameters governing each neuron, $\bth_i$, assuming each neuron is independent,  (iii) estimating the joint posteriors over all neurons,  $P[{\bf X}| \bF; \bth]$, and (iii) solving learning the functional connection matrix, $\bw$, by maximizing conditional expectation of  $\ln P[{\bf X}, {\bf F}| \bth^n]$.  Below, we describe each of these steps in detail.


\subsection{Estimating marginal posteriors over independent neurons using SMC, and learning their parameters}
\label{sec:methods:indep}

As stated above, our goal here is to derive an algorithm to efficient estimate $P[X(t), X(t-1) | F; \bth]$, which we refer to as the marginal posterior over each neuron (but note that it is also marginalized over all but nearest time bins).  As this was discussed at length in \cite{Vogelstein2009}, here we only provide a brief overview.  The standard forward-backward equations provide these posteriors, assuming the below integrals can be evaluated:

% \begin{align}
% 	P[X(t) | F(1), \ldots, F(t)] &= \frac{1}{Z} P[F(t)| X(t)] \int P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)] dX(t-1) \label{eqn:forward} \\
% 	P[X(t), X(t-1) | F] &= P[X(t) | F] \frac{P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)]}{\int P[X(t) | X(t-1)] P[X(t-1) | F(1), \ldots, F(t-1)] dX(t-1)} \label{eqn:backward}
% \end{align}
\begin{align}
	P[X_t | F_{1:t}] &= \frac{1}{Z} P[F_t| X_t] \int P[X_t | X_{t-1}] P[X_{t-1} | F_{1:t-1}] dX_{t-1} \label{eqn:forward} \\
	P[X_t, X_{t-1} | F] &= P[X_t | F] \frac{P[X_t | X_{t-1}] P[X_{t-1} | F(1), \ldots, F_{t-1}]}{\int P[X_t | X_{t-1}] P[X_{t-1} | F_{1:t-1}] dX_{t-1}} \label{eqn:backward}
\end{align}


\noindent where we have dropped the subscript $i$, replaced $(t)$ with $_t$, introduced notation that $X_{1:t}=(X(1),\ldots, X(t))$, and dropped the conditioning on the parameters for brevity sake.  Because the integral in Eq. \eqref{eqn:forward} cannot be analytically evaluated for our model, we approximate it with a sum using a sequential Monte Carlo (SMC) framework.  More specifically, we sample  from $x_t \sim P[X_t | X_{t-1}, F_{1:t}]$, and call each sample a ``particle''.  Given an entire particle swarm, we can compute the relative likelihood (or ``weight'') of each particle:

\begin{align}
	P[x_t | F_{1:t}] = \frac{1}{Z}\frac{P[F_t | x_t] P[x_t | x_{t-1}] P[x_{t-1} | F(1), \ldots, F_{t-1}]} {P[x_t | x_{t-1}, F_{1:t}]}
\end{align}

\noindent where $Z$ is a normalizing constant ensuring that $\sum P[x_t | F_{1:t}]=1$.  Given these weights, we resample to reduce the variance of the particles.  Recursing these three steps (sample, compute weights, resample) for $t=1, \ldots, T$ completes the approximation to Eq. \eqref{eqn:forward}.  We can now plug these approximations into Eq. \eqref{eqn:backward}, to recursively obtain particle approximations to marginal posteriors over neurons, $P[X_t, X_{t-1} | F]$.  Trivially, we can sum over $X_t$ to get marginal posteriors of $X_{t-1}$.

The sufficient statistics for estimating the parameters for each neuron, $\bth_i$, are these very marginal posteriors.  As shown in Eq. \eqref{eqn:loglik:definition-expl}, this maximization problem decouples into separate subproblems.  Specifically, the first term depends on only $\{\alpha_i, \beta_i, \gamma_i, \sigma_i\}$, which we can estimate by recursively solving a quadratic problem for $\{\alpha_i, \beta_i\}$ while holding $\{\gamma_i, \sigma_i\}$ fixed, and then holding $\{\alpha_i, \beta_i\}$ fixed, while estimating $\{\gamma_i,\sigma_i\}$.  Considering only the second term, we can estimate $\{\tau_i^c, A_i, C_i^b\}$ again using a quadratic solver, and use the residuals to estimate $\sigma_i^c$.  Note that all the parameters mentioned so far are constrained to be non-negative, but may be solved very efficiently using Matlab's \texttt{quadprog}, providing the appropriate constraints (and the gradients and Hessians, if desirable).  Finally, the last term, assuming neurons are independent,  may be expanded:

\begin{multline} \label{eqn:bw}
	E [\ln P[n_i(t), \bh_i(t) | \bF; \bth]] = P[n_i(t), h_i(t) | F_i] \ln (1-\exp\{-\exp\{b_i + \w_{ii} h_i(t)\}\Delta\})
	\\ +  (1-P[n_i(t), h_i(t) | F_i]) (-\exp\{b_i + \w_{ii} h_i(t)\}\Delta)] 
\end{multline}

\noindent which is concave in $\{b_i, \w_{ii}\}$, and may therefore be solved efficiently using any gradient ascent solver. In practice, we've found that imposing constraints on $\{b_i, \w_{ii}\}$ improves both the robustness and efficiency of estimating these parameters.  Our rational is that the double exponential is such a strong nonlinearity, that as the absolute value of these parameters approaches $\approx 10$, the likelihood becomes effectively flat.  We therefore restrict these parameters to be within $[-5,15]$, using Matlab's \texttt{fmincon}.

Our procedure therefore is to initialize the parameters for each neuron using some default values that we've found to be effective in practice, and then recursively (i) estimate the marginal posteriors (E step), and (ii) maximize the parameters (M step), using the above described approach.  We iterate these two steps until the change in parameters does not exceed some minimum threshold, $\eta^n$.  We can then use the marginal posteriors from the last iteration to seed our Gibbs sampling procedure described below, to obtain an estimate of $P[\bh(t) | \bF]$.

\subsection{Estimating joint posteriors over weakly coupled neurons}
\label{sec:methods:joint}

Computing the most likely estimates of the functional connectivity matrix, $\bw$, requires maximizing the third term of Eq. \eqref{eqn:loglik:definition-expl}, with respect to $\bw=\{w_{ij}\}$, which can be expanded:

\begin{multline} \label{eqn:bws}
	E [\ln P[n_i(t), \bh(t) | \bF; \bth]] = P[n_i(t), \bh(t) | F_i; \bth] \ln (1-\exp\{-\exp\{b_i + \sum_j \w_{ij} h_j(t)\}\Delta\})
	\\ +  (1-P[n_i(t), \bh(t) | F_i; \bth]) (-\exp\{b_i + \sum_j \w_{ij} h_j(t)\}\Delta)].
\end{multline}

\noindent This requires having the joint posterior, $P[\bX | \bF; \bth]$, over \emph{all} neurons.  However, the algorithm described in Section \ref{sec:methods:indep} merely provides the marginals over each neuron, $P[X_i(t) | F_i; \bth_i]$.  Therefore, in this section, we describe two approaches to infer the joint posteriors: one that makes an independence assumption, and one that is exact.

\subsubsection{Independent assumption for approximating the joint posteriors}

If the SNR in the calcium imaging is high,  the fluorescence data of each neuron provides nearly all the information about spike times.  Thus, the joint posterior approximately factorizes into a product of the marginal posteriors for each neuron:

\begin{equation} \label{eqn:indep_approx}
	P[{\bf X}(t)|{\bf F};\theta]  \approx \prod_{i=1}^N P[X_i(t)|F_i; \bth_i] = P[n_i(t)| h_i(t), F_i; \bth_i] \prod_{j=1}^N P[h_j(t)|F_j; \bth_j]. 
\end{equation}
	
\noindent Fortunately, we have already estimated all the above marginals using the SMC methods described above. Therefore, we sample spike trains and spike histories for each neuron (with replacement) from our particle swarm, $G(t)$, where $G(t)=\{X_i^{(l)}(t):  X_i^{(l)}(t) \sim P[X_i(t)|\{{\bf F}_i;\theta_i]$,  and then let our estimate of $P[n_i(t), \bh(t) | \bF; \bth]$ be the product of marginals for each sample.

We refer to such procedure for estimating the joint posterior as the independent approximation. Depending on the accuracy of such approximation, it may or may not be acceptable for the estimation of the true functional connectivity matrix $\bw$. However, as we show below, it is indeed the case that the independent approximation is adequate here for calcium imaging with reasonable SNR.  In such scenarios, we obtain a considerable speed-up in processing, not just because it obviates the need to generate joint samples, but also because we can parallelize the algorithm, inferring the marginals $P[X_i(t) | \bF_i; \bth_i]$ and estimating parameters $\bth_i$ for each neuron on a separate processor.  This will be required for estimating the functional connection matrix online, as will be discussed more in Section \ref{sec:discussion}.

\subsubsection{An exact sampling procedure to estimate the joint posteriors}

As mentioned above, the sufficient statistic for computing the maximum likelihood estimate of the functional connection matrix, $\bw$, is the joint posterior, $P[\bX(t) | \bF; \bth^n]$.  Unfortunately, the SMC approach described in Section \ref{sec:indep} does not scale well as the number of hidden states increases.  Therefore, our aim in this section is to develop an algorithm that builds on top of the SMC approach, that yields the sufficient statistics of interest.  A relative naive approach would be to use a vanilla Gibbs sampler to approximate the joint posteriors, of the form:

\begin{align}
	X_i(t) \sim P[X_i(t) | \bX_{\i}, X_i(1), \ldots, X_i(t-1), X_i(t+1), \ldots, X_i(T), \bF; \bth^n].
\end{align} 

\noindent Unfortunately, this approach is likely to mix very poorly, due to the strong temporal dependence between $X_i(t)$ and $X_i(t+1)$, for all $t$.  Instead, we propose to use a block-Gibbs strategy, sampling each spike train as a block:

\begin{align}
	X_i \sim P[X_i | \bX_{\i}, \bF; \bth^n].
\end{align} 

\noindent which is likely to mix quickly, given that spike trains are only weakly coupled. The SMC methods described above, however, yield only marginals over time, $P[X_i(t), X_i(t+1) | \bF; \bth]$, and are therefore insufficient.  We could sample the spike train from the set of particle swarms generated above (using a variant of finite forward-backward procedure, related to the Viterbi procedure \cite{RAB89}).  Such procedure, however, is known to result in biased samples \cite{Andrieu2007, NBR03}.  Specifically, such samples are distributed with the probability density

\begin{align}
X_i &\sim \frac{1}{Z(G)} P[X_i(t=1)| \bw]\prod\limits_{t=2}^{T}P[X_i(t)| X_i(t-1); \bw] \prod_{t=1}^{T} P[F_i(t)|X_i(t); \bth_i] \\
Z(G) &= \sum_{\{X_i(t)\}\in G} P[X_i(t=1)| \bw] \prod_{t=2}^{T-1}P[X_i(t)|X_i(t-1); \bw] \prod_{t=1}^{T}P[F_i(t)|X_i(t); \bth_i].
\end{align}

\noindent where $G=\prod\limits_t G(t)$ is the collection of particle swarm samples $G(t)=\{X_i^{(l)}(t):  X_i^{(l)}(t) \sim P[X_i(t)|\{{\bf F}_i;\theta_i]$.  In particular, such probabilities of different sequences differ from the true probabilities by the difference in the estimated normalization constant $Z(G)$ from the true normalization $Z$.  This bias may be removed by embedding SMC into a larger importance sampling algorithm, correcting for bias $Z(G)$ by retaining SMC samples with probability $\sim Z(G)$ \cite{Andrieu2007}. In particular, Andrieu et al. \cite{Andrieu2007} show that as the size of the particle swarm grows the acceptance ratio of such importance sampling tends to unity.

A different approach, developed by Neal et al. \cite{NBR03}, is to use Markov Chain Monte Carlo method (MCMC) with the Markov chain constructed specifically to have the necessary equilibrium distribution $P[\bX| {\bf F}; \theta]$. The Markov Chain is constructed as follows. First, continuous-state HMM is replaced with a discrete HMM on the grid, $G=\prod G(t)$. At each time-point $t$, we therefore define a pool of $L$ grid-points $\{X_i^{(l)}(t)\}$ drawn independently from a given proposal density $\rho_i^t[X_i(t)]$. The grid $G$ is then constructed as a direct product of such pools.  Second, sequence of states is selected over such grid with the probability

\begin{equation}\label{eqn:nealprob}
X_i\sim P[X_i(t=1)|{\bf X}_{\i}; \bth_i]\prod\limits_{t=2}^{T}P[X_i(t)|X_i(t-1), {\bf X}_{\i}; \bth_i] \prod\limits_{t=1}^{T} \frac{P[F_i(t)|X_i(t); \bth_i)}{\rho_i^t[X_i(t)]}.
\end{equation}

This may be done directly and efficiently using forward-backward procedure with the observation probability modified to $P[F_i(t)|X_i(t), \bth_i]\rightarrow P[F_i(t)|X_i(t), \bth_i)/{\rho_i^t[X_i(t)]}$.  XXX Y: i'm not sure what you mean here.  XXX

Finally, states from the chosen sequence of states $X_i$ should be included in the pools $G(t)$ for the next MCMC step, $X_i(t)\in G(t)$, and the above two steps should be repeated with the new grid $G$. It is shown in \cite{NBR03} that the limiting distribution of such Markov chain is the correct unbiased distribution

\begin{equation}
X_i \sim P[X_i(t=1)|{\bf X}_{\i}; \bth_i]\prod\limits_{t=1}^{T-1}P[X_i(t)|X_i(t-1), {\bf X}_{\i}; \bth_i]\prod\limits_{t=1}^{T}P[F_i(t)|X_i(t); \bth_i].
\end{equation}

The advantage of Neal's method (over embedding SMC into an importance sampler) are that (i) the grids $G$ are simple to prepare, and (ii) no importance sampling rejection step is required, as such a  step is implicitly accommodated in the forward-backward procedure via modified observation probability Eq. \eqref{eqn:nealprob}.

The proposal density, $\rho_i^t[X_i(t)]$, may be chosen arbitrary as long as it has sufficiently large support.  To achieve faster convergence, we use marginal densities $P[X_i(t)|{\bf X}_{\i}, \bf F; \bth_i]$ computed from the conventional SMC algorithm. Such an efficient proposal density allows the Markov Chain to converge extremely quickly.  More specifically, we let $\rho_i^t[X_i(t)] = \rho^i_t[C_i(t)] \rho^i_t[n_i(t)] \rho^i_t[h_i(t)].$  The proposals for calcium, $\rho^i_t[C_i(t)]$, were mixtures of Gaussians centered on particles from the particle swarm $C_i^{(l)}(t)$ with the variances $\approx var\left[C_i^{(l)}(t)-C_i^{(l')}(t) \right]$. $\rho^i_t[n_i(t)]$ was taken to be Bernoulli distribution with the spike probability estimated from the particle swarm. Finally, $\rho_i^t[h_i(t)]$ XXX ? XXX.  Such samples for single neurons from the conditional probability distributions $P[X_i|{\bf X}_{\i}, \bf F; \theta]$ may be subsequently used in block-Gibbs sampling procedure to acquire joint sample from $P[{\bf X}| {\bf F}; \theta]$.  We repeat the MCMC procedure to sample blocks of one neuron state-sequence at a time $X_i\sim P[X_{i}|{\bf X}_{\i},{\bf F}; \bth_i]$, sequentially for all neurons $i=1\ldots N$ for $N_G$ Gibbs cycles (in practice, we typically take $N_G \approx 10$).  

Importantly, this hybrid MCMC-Gibbs procedure is sufficiently general to deal with arbitrary spike history terms.  More specifically, although we only require $P[\bX(t) | \bF; \bth]$ (i.e., the joint posterior marginalized over time), we obtain $P[\bX | \bF; \bth]$, the full joint posterior.  Thus, for the above model, upon obtaining the full joint posterior, we marginalize over time.  We could, however, have let the probability of a spike from a given neuron be governed by:

\begin{align}
n_i(t) \sim f(b_i+\sum_{j=1}^{N}\sum_{t'<t} w_{ij}(t')).
\end{align}

\noindent Note that this is a generalization with respect to Eqs. \eqref{eqn:glm:definition} and \eqref{eqn:h:definition} in two ways.  First, we no longer impose Markovian dynamics on the spike histories.  Second, the shape of the PSP due to a presynaptic spike is \emph{synapse} specific, whereas in the above model we assumed that the PSP shape was identical for all synapses sharing a common presynaptic neuron (i.e., spike history terms were indexed only by $j$, not also $i$).  

\subsection{Estimating the functional connectivity matrix} \label{sec:methods:parameters HMM}

Computing the maximum likelihood estimates of the functional connectivity matrix, $\bw$, is an optimization problem with $N^2$ variables. By construction, however, log-likelihood is convex in $\bw$, and, log-likelihoods for different neurons, Eq. \eqref{eqn:bws}, are independent and may be maximized separately. Estimating $\bw$ thus involves solving of $N$ separable $N+1$-dimensional convex optimization subproblems (the $+1$ arises because the baseline parameter, $b_i$, is coupled to the weight parameters), which can be done efficiently using standard algorithms such as Newton-Rapson methods.We used standard Matlab's nonlinear optimization function \texttt{fmincon}, provided in optimization toolbox, to solve this problem for up to $N=500$ neurons.  Constraints are important here, because the likelihood flattens as $|\w_{ij}|$ increases, and numerical error (for double precision) starts dominating.  Therefore, we impose the constraint that $|\w_{ij}|<10$ for all $i,j$. While this approach yields the maximum likelihood estimate for the connectivity matrix, simple properties of the connectivity matrix, that may be known a priori, may be extremely helpful in obtaining accurate solutions. For instance, it is commonly believed that connectivity in many neuroanatomical substrates is ``sparse'', meaning that most neurons form synapses with only a small fraction of their neighbors \cite{??}.  Further, Dale's Law, which states that each of a neuron's bouton's release the same neurotransmitter, and therefore, its postsynaptic synapses have the same sign (e.g., positive).  Below, we discuss how to incorporate these priors to improve our inference.

\subsubsection{Imposing a sparse prior on the functional connectivity}

Enforcing sparseness for signal recovered with a series of linear measurements via $L1$-regularizer is known to dramatically reduce the amount of data necessary to accurately reconstruct the signal \cite{Candes2005, DE03, Mishchenko2009}. Although here the estimation problem is not linear, it is interesting what impact analogous prior might have on the reconstruction of the functional connectivity matrix $\bw$. By introducing exponential prior to our GLM model\cite{Stevenson2009}, we can find the maximum a posteriori estimate for the connectivity matrix:

\begin{equation}\label{eqn:likelihoodsparseGLM}
\hbw_i^{sparse} = \argmax_{\bw_i} E[\ln P[n_i, \bh |{\bf F}; \bth]] P[\bw]]= \argmax_{\bw} E[\ln P[n_i, \bh |{\bf F}; \bth]] - \lambda \sum_{i,j}|\w_{ij}|,
\end{equation}

\noindent where the exponential prior parameter $\lambda$ may be set from a priori neuroanatomical or neurophysiological data.  By introducing slack variables $s_{ij}(t)>|\w_{ij}(t)|$, this problem may be converted to a nonlinear program that can be solved using the interior point method:

\begin{equation} \label{eqn:conconvexopt}
% \begin{array}{l}
	\hbw_i^{sparse} = \argmax_{\substack{\w_{ij}<s_{ij} \forall j \\ -\w_{ij}<s_{ij} \forall j}}   E[\ln P[n_i, \bh |{\bf F}; \bth]] - \lambda \sum_{i,j} s_{ij}
%\min \left\{-\sum\limits_t E\left[ n_i(t) J_i(t) - (1-n_i(t)) \exp(J_i(t)) \Delta \right]+\lambda \sum\limits_{t'j}s_{ij}(t')\right\}
%, \text{ s.t. } \\ \w_{ij}<s_{ij}, \; -\w_{ij}<s_{ij}\; \forall j.
% \end{array} 
\end{equation}

\noindent This sparse prior does not change the convexity of log-likelihood for $\bth^n$, so this regularized problem may still be solved efficiently.  We used Matlab's standard function \text{fconmin}, provided in optimization toolbox, to solve this constrained optimization problem for up to $N=500$ neurons. As we will see below, sparse prior dramatically decreases the amount of data necessary for accurate estimation of the connectivity matrix.

\subsubsection{Imposing Dale's law on the functional connectivity}

As neurons seem to abide by Dale's law, it is natural for us to constraint our estimates of the functional connection matrix accordingly. In terms of the connectivity matrix, Dale's law translates into the condition of sign-constancy of the matrix columns. Dale's law is easy to enforce by constraining $\w_{ij}$ to be either positive or negative for given $j$.  Sign assignments may be chosen by inspecting unconstrained solution $\bw$ and choosing the column $\w_{\ast j}$ to be excitatory if the sum-squares of the positive terms $\w_{ij}$ for given $j$ in the unconstrained solution is greater than that of the negative terms, and inhibitory otherwise.  After that, Dale's law may be enforced by constraining the weights:


\begin{align}
	\hbw_i^{dale} = \argmax_{\substack{\w_{ij}<0 \forall \; j \in \mathcal{I} \\ \w_{ij}>0 \forall \; j \notin \mathcal{I} }}   E[\ln P[n_i, \bh |{\bf F}; \bth]]
\end{align}

% \begin{equation}
% \begin{array}{l}
% \min \left\{-\sum\limits_t E\left[ n_i(t) J_i(t) - (1-n_i(t) \exp(J_i(t)) \Delta \right]+\lambda \sum\limits_{t'j}s_{ij}(t')\right\}, \text{ s.t. }\\
% \w_{ij}(t')<0, \; -\w_{ij}(t')<s_{ij}(t') \; \forall j, t'\text{ where }j\text{ is inhibitory neuron}, \\
% \w_{ij}(t')<s_{ij}(t'), \; -\w_{ij}(t')<0 \; \forall j, t'\text{ where }j\text{ is excitatory neuron}. \\
% \end{array}
% \end{equation}

\noindent where $\mathcal{I}$ is the set of inhibitory presynaptic neurons.  While this optimization problem is not convex (due to the hard constraints), it may still be approximately solved using the same methods as above.  Imposing both Dale's and the sparse prior on the connectivity weights thus follows straightforwardly.  

\subsection{Specific implementation notes} \label{sec:methods:specific_implementation}


We break the inference problem of estimating the functional connection matrix given only fluorescence data into a few  steps (see Algorithm \ref{eqn:pseudocode}).  First, we estimate the parameters $\bth_i$ assuming each neuron is independent on a subset of data with $O(10)$ spikes, using the SMC methods described in Section \ref{sec:methods:indep}.  This ``inner loop'' is a set of $N$ EM algorithms, iterating between inferring expected spike trains and estimating parameters. As the number of parameters for each $\bth_i$ scales linearly with the number of neurons (it does not include connection weights $w_{ij}$ for $i\neq j$), these parameters may be estimated using a relatively amount of data.  Second, given $\bth_i$ for each neuron, we approximate the joint posteriors, $P[X_i(t), \bh(t) | \bF; \bth]$, by sampling using either the independent approximation or the exact hybrid MCMC-Gibbs method, as described in Section \ref{sec:methods:joint}.  Finally, given the estimated joint posteriors $P[X_i(t), \bh(t) | \bF; \bth]$, we estimate presynaptic connectivity weights, $w_{i \ast}$, for each neuron.  All three steps are iterated until $\bw$ converges. Table \ref{eqn:pseudocode} provides pseudocode for this approach.  

\begin{algorithm}
\caption{Pseudocode for estimating functional connectivity from calcium imaging data using EM. Note that $\eta^n$, $\eta^F$, $N_G$ are somewhat arbitrarily chosen bounds.  XXX do we ever actually do this outer loop more than once? if so, i don't see why it would help, unless the inferred spike trains from the joint samples were a big improvement of the independent samples, which i thought didn't happen XXX}
\label{eqn:pseudocode}
\begin{algorithmic}
\While{$|{\bw}^{(l)}-{\bw}^{(l-1)}|>\eta^w$}
  \ForAll{$i=1\ldots N$}
    \While{$|{\bth_i}^{(l)}-{\bth_i}^{(l-1)}|> \eta^F$}
      \State Approximate $P[X_i|{\bf F}_i; \bth_i]$ using SMC
      \State Maximize ${\bth_i}^{(l+1)}=\argmax_{\bth_i} E\left[\ln P[X_i | F_i; \bth_i] \right]$
    \EndWhile
  \EndFor
  
  % \For{$k=1\ldots N_G$}
    \ForAll{$i=1\ldots N$}
      \State Approximate $P[X_i(t), \bh(t) |{\bf F}; \bth]$ using either independent approximation or hybrid MCMC-Gibbs
    \EndFor
  % \EndFor 

  \ForAll{$i=1\ldots N$}
  	\State Maximize ${\bth^n_i}^{(l+1)}=\argmax_{\bth^n_i} E\left[\ln P[X_i, \bh | \bF; \bth]\right]$  
  \EndFor

\EndWhile
\end{algorithmic}
\end{algorithm}

An important feature of the above algorithm is that the above procedures straightforwardly parallelize. Estimation of  $\bth_i$ could be done independently for all neurons. Calculation of the functional connectivity matrix $\bw$ also involved solving $N$ optimization subproblems for different neurons that could be done independently. In the independent approximation, the posterior $P[X_i(t) | \bF; \bth_i]$ could be obtained in parallel for different neurons.  The hybrid MCMC-Gibbs method for estimating the posteriors could also be parallelized by drawing HMM state-sequences within Gibbs loop for a few neurons at a time, instead of single neuron at a time. XXX Y: i don't really know what you mean here. XXX High parallelizability of these steps results in a significant time savings when analysis of calcium imaging data was performed on multi-processor computer or a cluster.  We performed bulk of the calculations on a high-performance cluster of Intel Xeon L5430 based computers (2.66 GHz). For 10 minutes of simulated fluorescence data, imaged at $30$ Hz, calculations typically took 10-20 minutes per neuron using independent approximation, with time split approximately equally between (i) estimating $\bth_i$,  (ii) approximating the posteriors using the independent sampler, and (iii) estimating the functional connectivity matrix, $\bw$. The hybrid MCMC-Gibbs sampler was substantially slower, up to an hour per neuron per Gibbs pass, with Gibbs sampler being the most computationally expensive part. Parallel computation made calculations for large populations of neurons $N\approx 200-500$ possible.

\subsection{Accuracy of the estimates and Fisher information matrix} \label{sec:methods:accuracy_Fisher}

To determine the necessary amount of data for accurate estimation of the functional connectivity matrix, we calculate Fisher information for $P[\bw| \bX]$. Assuming for simplicity perfect knowledge of spike trains (i.e. such not corrupted by inference errors from calcium imaging) and single time-bin coupling, i.e. $h_{j}(t)\neq 0$ only for time-delay $t=1$, we write the Fisher information matrix as:

\begin{equation}
\begin{array}{rl}
C^{-1}=\frac{\partial (-\ln P)}{\partial \w_{ij}\partial \w_{i'j'}}
=-&\delta_{ii'}\sum\limits_t\left[
n_i(t)n_{j}(t-1)n_{j'}(t-1)\left(-\frac{f'(J_i(t))^2}{f(J_i(t))^2} +
\frac{f''(J_i(t))}{f(J_i(t))}\right) \right. \\
&\left.-\Delta (1-n_i(t))n_{j}(t-1)n_{j'}(t-1)f''(J_i(t))\right].
\end{array}
\end{equation}

\noindent where $f'$ and $f''$ correspond to the first and second derivatives of our linking function (c.f Eq. \eqref{eqn:glm:definition}), and $\delta_{ii'}$ is XXX ? XXX.  When $f(J)=\exp(J)$ XXX Y: we don't use an exponential here.  is it worth modifying this accordingly? XXX, and coupling between spikes is week, this may be rewritten as:

\begin{equation}\label{eqn:fisher}
\begin{array}{rl}
C^{-1}
&=\delta_{ii'} (T\Delta) P[n_i(t)=0, n_j(t-1)=1, n_{j'}(t-1)=1]E[f(J_i(t))|n_i(t)=0, n_j(t-1)=1, n_{j'}(t-1)=1] \\
&\sim (T\Delta)\left[(r \tau_w)\delta_{ii'}\delta_{jj'}+O((r \tau_w)^2)\right]r.
\end{array}
\end{equation}

Here $(T\Delta)$ is the total observation time, $ \tau_w$ is ``the coincidence time'' --- the typical EPSP/IPSP time-scale over which the spike of one neuron affects the spike probability of the other neuron --- and $r \approx E[f(J_i(t))|n_i(t)=0, n_j(t-1)=1, n_{j'}(t-1)=1]$ is the typical firing rate.  For successful determination of the functional connectivity matrix $\bw$, the variance $C$ should be smaller than the typical scale $\langle \bw^2\rangle$, i.e.

\begin{equation}
(T\Delta) \sim (\bw^2 r^2  \tau_w)^{-1}.
\end{equation}

For typical values of $\bw^2\approx 0.1$, $r\approx 5$  Hz and $ \tau_w \approx 10$ msec, 
with this order of magnitude estimate we obtain $T$ of the order of hundred seconds. This theoretical estimate of the necessary amount of fluorescent data is in good agreement with our simulations below.

Note also that necessary recording time does not depend on the number of neurons in the imaged network $N$. This unexpected result is the direct consequence of the special form of $C^{-1}$ in Eq. \eqref{eqn:fisher}. In particular, when $r \tau_w <<1$, this matrix is dominated by the diagonal term $(T\Delta)(r^2  \tau_w)$, and so the Fisher information matrix is predominantly diagonal with the scale $(r^2 \tau_w T\Delta)^{-1}$, independent of the number of neurons $N$. This theoretical result is also directly confirmed in our simulations below.