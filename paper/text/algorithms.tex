XXX: this section is still under much development


\subsection{Default algorithm: see \cite{BJ08} for details} \label{sec:default}

\begin{description}
\item[Initialization]: \\
\begin{enumerate}
\item Manually define an ROI around each neuron.
\item Average pixel intensity from all pixels in a frame to obtain $F_{i,t}$, i.e., the fluorescence intensity indexed by cell indentity $i$ and frame number $t$.
\item Separately for each neuron, implement PFS as described in \cite{BJ08} (ie, with all cross-coupling terms turned off). 
\end{enumerate}
\item[Recursion]: \\
\begin{enumerate}
\item For each neuron, let the input, $\bu_{i,t} = [\bs_t, \{\widetilde{\h}_{i,t}\}]$, where $\{\widetilde{\h}_{i,t}\}= \{E[\h_{i,t} | \bF]$ for all $i\}$.
\item Use PFS from \cite{BJ08} to learn $\ve{\omega}$ 
\end{enumerate}
\end{description}

\subsection{Population PF} \label{sec:PopPF}

\begin{enumerate}
\item same as \ref{sec:default}
\item same as \ref{sec:default}
\item particle filter step: this generalizes \ref{sec:default} by sampling not just on self-histories, but also cross-histories.  because the transition distribution factorizes as above, we can sample from the optimal one observation ahead sampler, ie, $p(\x_t | \x_{t-1}, \y_t)$ (by approximating the likelihood term as a Gaussian in $\Ca_{i,t}$). in particular, to sample spikes after integrating out everything, one is left with:

\begin{align}
n_{i,t} \sim p(n_{i,t} | \h_t) \mathcal{N}(\Ca_{i,t}; \mu_{c_{i,t}}, \sigma_{c_{i,t}}^2)
\end{align}  

\noindent where $\mathcal{N}(\Ca_{i,t}; \mu_{c_{i,t}}, \sigma_{c_{i,t}}^2$ is a Gaussian approximation to the product of the likelihood and calcium update terms.  

\item same as \ref{sec:default}
\item estimate parameters using standard EM stuff as in \cite{BJ08}
\end{enumerate}

it's not obvious whether this approach will outperform the approach outlined in \ref{sec:default}

\subsection{Gibbsish Population PF} \label{sec:G1PF}

this generalizes \ref{sec:PopPF}, by sampling conditioned not just on spike histories, but also on spike futures from other neurons.  in particular, we want to sample spikes for one neuron conditioned on its own past spikes, and \emph{all} spikes from all other neurons.  having already completely an iteration, we now sample spikes according to:
\begin{align}
n_{i,t} \sim p(n_{i,t} | \h_t, \h_{\backslash i, t+1}) \mathcal{N}(\Ca_{i,t}; \mu_{c_{i,t}}, \sigma^2_{c_{i,t}})
\end{align}

\noindent where $\h_{\backslash i,t+1}=\{h_{1,t+1}, \ldots, h_{i-1,t+1}, h_{i+1,t+1}, \ldots, h_{N,t+1}\}$.  we compute this using the method described in \cite{PillowLatham07}.  For future spikes, we let the posterior mean of the spike train from the previous EM iteration correspond to future spikes.

\subsection{Gibbser Population PF} \label{sec:G2PF}

this generalizes \ref{sec:G1PF}, by sampling not just sampling based on previous EM iteration's mean posterior spike train, but actually doing a proper gibbs step within each forward pass. in particular, we do the sampling as described in \ref{sec:G1PF} to initialize, and obtain a spike train for each neuron.  we then iterate, sampling for each neuron, conditioned on the entire spike trains for other neurons.  by gibbs, we will eventually converge to having sampled jointly from them all. i am not convinced that this is a particularly useful generalization of the \ref{sec:G1PF}.


\subsection{Metropolis-Gibbs Population PF} \label{sec:G3PF}

this generalizes \ref{sec:G2PF}, by placing the Gibbs sampler just described into a MCMC approach.  more specifically, we proceed as follows. for each neuron:

\begin{enumerate}
\item generate $M-1$ particles, sampling according to $p( n_{i,t} | \h_t, \h_{\backslash i, t+1}, F_{i,t})$ as described in \ref{sec:G2PF}.  
\item add current path to population of particles and compute appropriately normalized transition probabilities
\item use standard forward-backward sampling algorithm for HMM's to sample $\x^{\ast}$ from this augmented space
\item compute $q(\x^{\ast})$, the probability of sampling $z$ using a forward-backward recursion
\item compute probability of acceptance, $r= \frac{q(\x) p(\x^{\ast})}{q(\x^{\ast})p(\x)}$, where $\x$ is the current path and $p(\x^{\ast})$ is the posterior
\end{enumerate}

we iterate this step until we accept a new spike train for a particular neuron, and then repeat for all subsequent neurons. alternately, we could randomly choose a neuron after each iteration.  i have no idea which approach would be better.

\paragraph{speeding things up}

as this probably will take a while, we can do a number of things to speed it up

\begin{itemize}
\item skip generating particles step sometimes, ie, keep sampling from a particular HMM until prob of acceptance gets too small
\item do an increment/decrement thing.  in other words, after generating the first set of particles, with each additional acceptance, we simply eliminate a path (based on its likelihood).  thus, we must only generate $M-1$ extra particles once.
\item instead of using the standard hmm sampling to generate a new proposal, use Viterbi for PF (as described in \cite{GodsillWest01}, or the fast version of that approach (as described in \cite{KlaasFreitas05} to generate a possible new sequence. then, if that one is not accepted, use standard hmm sampling to generate other proposals.
\end{itemize}

\subsection{Ways we might make stuff better}

these ideas potentially improve on all the above ideas.

\begin{enumerate}
\item Improved observation model --- Identify ROI for each neuron using some ImageJ plug-in (or other such method).  Instead of equally weighting all pixels, initialize pixel weights with first eigenvector computed using SVD, and then proceed as before.  upon constructing SS, update spatial filter weights along with other parameters.  An analysis of the kind of spatial filters should facilitate parameterizing the filter (potentially as a mixture of Gaussians).  Alternately, an appropriate regularizing kernel may help. Note that if two or more neurons are in the same ROI, the observation model must be modified somewhat (but I'm not putting those details in here at this time) 
\item Improved transition model --- The most obvious improvement would be to allow additional time constants for the dynamics (either calcium or fluorescence).  Perhaps the best way to decide what to include would be to take data for which we have both images and ephys, and fit a couple parametric models, and do cross-validation to compare which is best.  The options would include: (i) an additional calcium state, (ii) temporal dynamics for the fluorescence, (iii) no noise on calcium, or (iv) some combination of the above.  
\item Rao-blackwellized Particle Filter (RBPF) --- This would amount to having a mixture of kalman filters (MKF), each conditioned on a particular spike train.  As far as reducing unnecessary variance, I think it would be hard to justify not incorporating this into our model (see \cite{Chen00} for details). Note, however, that in the limit as $N \rightarrow \infty$, the particle approximation is perfect, whereas the RBPF (or MKF) is not, because we must estimate the likelihood as a Gaussian function of $\Ca$.  
\end{enumerate}

\subsection{these are not very good ideas, but i thought they were potentially worth writing down}

\begin{enumerate}
\item Improved resampling --- we could use a number of tricks here
\begin{itemize}
\item  sample $N_p' > N_p$ particles, but only resample using $N_p$ particles, aka, prior boosting (see \cite{Green95,CarpenterFearnhead99} for details).
\item  Rejection sampling --- We do not resample, but rather, only accept particles at each time. See \cite{Chen03} Section E for a summary  (and refernces therein for details). 
%first, we sample $\x_{t-1}^{(i)}$ with probability $\gamma_t^{(i)}$, where
%\begin{align}
%\gamma_t^{(i)} &= \frac{C_t^{(i)}}{N_p C_n}
%C_n &\approx \frac{1}{N_p^2} \sum_{i,j=1}^{N_p} p(\y_t | x_{t|t-1}^{(ij)})
%C_n^{(i)} \approx \frac{1}{N_p} \sum_{j=1}^{N_p} p(\y_t | \x_{t|t-1}^{(ij)})
%\end{align}
%
%\noindent and then generate a new sample, $\z_t$, from $q(\cdot)$, which is accepted only if $u\leq \alpha(\z_t)$, where 
%\begin{align}
%\alpha(\z_t) = \frac{p(\y_t | \z_t) p(\z_t | \x_{t-1}^{(i)}) / q(\z)} {\sup_{\z} \{p(\y_t | \z_t) p(\z_t | \x_{t-1}^{(i)}) / q(\z)\}}
%\end{align}
%\noindent where $\z_t$ is the newly sampled particle, $q(\cdot)$ is the proposal distribution, and $u \sim \mathcal{U}(0,1)$
\item  robust weighting (to handle outliers) (see review on particle filters)
\end{itemize}  

\item MCMC with particle filtering --- there are a number of ways to do this kind of thing
\begin{itemize}
\item RESAMPLE-MOVE --- 
\item Metropolized Gibbs sampler --- see \cite{Liu96} for details
\end{itemize} 

\item data-augmentation for parameter estimation (ie, iterate estimating $p(\x | \y, \thet)$ and $p(\thet | \y, \x)$ to eventually converge to $p(\x, \thet, \y)$).
\item use algorithm in continuous-time particle-filter paper (\cite{NgDearden05}) for parameter estimation
\end{enumerate}

%\paragraph{thoughts from liam}
%
%instead of computing $p(n_t,n_{t-1}|Y)$ (the sufficient statistics for
%		EM), we need to compute more general functions of the join spike
%train.  Thus we need to sample.  we can use methods similar to those
%described above.
%
%We will construct a Gibbs-Metropolis sampler \cite{RC05}: if we can
%sample from $p(n_i|Y,\{n_{\backslash i}\})$ sequentially, then we can
%compute samples from $p(\{n_i\}|Y)$, as desired (this is the standard
%		Gibbs sampler idea \cite{GG84}).  Unfortunately, even sampling from
%$p(n_i|Y,\{n_{\backslash i}\})$ is not directly possible; thus we use
%the Metropolis-Hastings (M-H) algorithm to construct an approximate
%sampler whose properties can be guaranteed to be correct if the
%algorithm is run long enough.
%
%The key to making the M-H algorithm work efficiently (as with any
%		Monte Carlo method) is to define a good proposal density: the proposal
%should be both 1) close to the target density $p(n_i|Y,\{n_{\backslash
%	i}\})$ and 2) easy to sample from.  We combine two ideas to obtain a
%	good proposal density here.  First, \cite{PL07} introduced a good
%	proposal density for sampling spike trains in the GLM given
%	observations of other neurons; the basic idea is to modify the rate of
%	the target neuron to ``look ahead'' at the spike train of other
%	neurons in the network (since the spikes from the target neuron at the
%			current time will affect the future spike trains of the connected
%			neurons), by incorporating forward terms into the GLM instead of just
%	the standard spike history filters; while we do not have space to
%	reproduce the argument here, it may be shown that their proposal may
%	be justified as an approximation to a sequential Gibbs sampler for
%	$p(n_i|\{n_{\backslash i}\})$.  This idea is easy to incorporate in
%	the present context: we simply use the the modified firing rate from
%	\cite{PL07} as our prior firing rate $\lambda_i(t)$.
%
%	Second, while exact sampling from state-space models with infinite
%	state spaces is difficult in general, it is well-known that the
%	particle filter may be used to generate \emph{approximate} conditional
%	samples, using an efficient forward-backward recursion \cite{DFG01};
%as usual, in the limit of a large number of particles, this
%approximate sampling algorithm becomes exact.
%
%Thus, combining these two ideas, we will use our particle filter (with
%		the modified firing rate $\lambda_i(t)$) to form a proposal density
%$q(n_i,C | Y,n_{\backslash i})$: we run the particle filter forward to
%create a list of particle positions, then append the current path to
%this list of positions and then apply the standard discrete Markov
%chain forward-backward sampling algorithm (with transition
%		probabilities obtained by simply restricting the original state-space
%		transition probabilities to this finite list of states) to obtain the
%new proposed sample path.  Since the current path is included in the
%list of particle positions, we may compute the M-H transition
%probability (defined as the ratio of the transition probabilities to
%		and from the current and proposed sample path, multiplied by the
%		likelihood of the sample paths evaluated under the model) and
%therefore obtain the M-H acceptance probability, guaranteeing the
%correctness of the samples.  The key point is that these computations
%are all recursive, and therefore require just $O(T)$ time.  Moreover,
%	 as the number of particles becomes large enough, and if spike
%	 interaction terms become weak, the probability of acceptance tends to
%	 one (i.e., the sampler becomes exact), ensuring that the
%	 Metropolis-Hastings chain mixes quickly, and making the algorithm
%	 efficient.  (A similar embedded Markov chain sampler was proposed by
%			 \cite{NBR03}, though these authors did not note the benefits of using
%			 the particle filter to efficiently choose the state space of the
%			 embedded Markov chain.)
%
%	 In practice, to keep the acceptance rates in the Metropolis algorithm
%	 reasonable we may apply the Gibbs idea and break up the spike train
%	 into a few temporal chunks, and apply the Metropolis algorithm
%	 Gibbs-style on each of these chunks.
%
%	 note that each of these two steps parallelizes nicely: sampling from
%	 two spike trains $p(n_i|Y,C,\{n_{\backslash i}\})$ and
%	 $p(n_{i'}|Y,C,\{n_{\backslash i'}\})$ can be done independently of
%	 each other as long as the corresponding glm terms $h_{ij}$ are zero.
